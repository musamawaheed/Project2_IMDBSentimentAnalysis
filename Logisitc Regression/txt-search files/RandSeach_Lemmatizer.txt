Tested on whole training data
parameters = {'tfidf__lowercase': [True, False], 'tfidf__max_features': [30000, 50000, 100000, None], 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4), (2, 3)], 'tfidf__smooth_idf': [True, False], 'tfidf__stop_words': ['english', stop_words, None], 'tfidf__strip_accents': ['ascii', 'unicode', None], 'tfidf__tokenizer': [LemmaTokenizer(), StemmTokenizer(), None], 'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]}


rand = RandomizedSearchCV(pipe, parameters, cv=5, scoring='accuracy', n_iter=100, random_state=5, n_jobs=1)





Best parameters set found on development set:

{'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 100} 0.8645333333333334

Rand scores on development set:

0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.541 (+/-0.022) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.513 (+/-0.006) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.855 (+/-0.019) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.513 (+/-0.005) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.815 (+/-0.040) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.750 (+/-0.050) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.1}
0.851 (+/-0.031) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.516 (+/-0.009) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.578 (+/-0.036) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.562 (+/-0.031) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.827 (+/-0.053) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.851 (+/-0.028) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.858 (+/-0.015) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.844 (+/-0.051) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 100}
0.859 (+/-0.034) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.823 (+/-0.044) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.819 (+/-0.023) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 100}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.525 (+/-0.012) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.571 (+/-0.031) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.850 (+/-0.034) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 10}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.756 (+/-0.068) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.1}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.817 (+/-0.034) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.854 (+/-0.028) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.863 (+/-0.020) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 100}
0.814 (+/-0.021) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.795 (+/-0.028) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.514 (+/-0.005) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.830 (+/-0.041) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.822 (+/-0.051) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.855 (+/-0.048) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.525 (+/-0.013) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.814 (+/-0.032) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 100}
0.850 (+/-0.026) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.813 (+/-0.037) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.809 (+/-0.042) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.852 (+/-0.014) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.797 (+/-0.033) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.813 (+/-0.030) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.530 (+/-0.015) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.831 (+/-0.035) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.820 (+/-0.032) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.826 (+/-0.033) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.713 (+/-0.081) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 1}
0.820 (+/-0.054) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.794 (+/-0.047) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.800 (+/-0.044) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.753 (+/-0.035) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 0.1}
0.846 (+/-0.040) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.854 (+/-0.033) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.770 (+/-0.042) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.793 (+/-0.044) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.859 (+/-0.024) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.813 (+/-0.040) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.822 (+/-0.030) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.836 (+/-0.025) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.543 (+/-0.025) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.509 (+/-0.001) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 0.001}
0.793 (+/-0.046) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.781 (+/-0.043) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.859 (+/-0.018) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.843 (+/-0.047) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 100}
0.820 (+/-0.066) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 10}
0.824 (+/-0.033) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.542 (+/-0.027) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.812 (+/-0.051) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.856 (+/-0.028) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 10}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': True, 'logreg__C': 0.01}
0.797 (+/-0.039) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 1}
0.517 (+/-0.010) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 0.01}
0.838 (+/-0.035) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 1}
0.701 (+/-0.066) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': None, 'tfidf__lowercase': False, 'logreg__C': 0.1}
0.815 (+/-0.035) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (2, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.865 (+/-0.027) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 100}
0.823 (+/-0.050) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 1}
0.851 (+/-0.031) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 100000, 'tfidf__lowercase': True, 'logreg__C': 10}
0.767 (+/-0.084) for {'tfidf__tokenizer': <__main__.StemmTokenizer object at 0x000002399364C438>, 'tfidf__strip_accents': None, 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.853 (+/-0.039) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': None, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 100000, 'tfidf__lowercase': False, 'logreg__C': 100}
0.787 (+/-0.048) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': 'unicode', 'tfidf__stop_words': 'english', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__max_features': 30000, 'tfidf__lowercase': False, 'logreg__C': 0.1}
0.509 (+/-0.001) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 30000, 'tfidf__lowercase': True, 'logreg__C': 0.001}
0.751 (+/-0.049) for {'tfidf__tokenizer': <__main__.LemmaTokenizer object at 0x000002399364C3C8>, 'tfidf__strip_accents': 'ascii', 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 3), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 0.1}
0.835 (+/-0.029) for {'tfidf__tokenizer': None, 'tfidf__strip_accents': None, 'tfidf__stop_words': ['in', 'of', 'at', 'a', 'the', 'an', 'is', 'that', 'this', 'to', 'for'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 50000, 'tfidf__lowercase': True, 'logreg__C': 1}
[Finished in 4822.9s]